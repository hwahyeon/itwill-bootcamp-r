wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
## 6. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal)#, family="malgun")
hate <- file(file.choose(), encoding="UTF-8")
head(hate)
hate
hate <- file(file.choose(), encoding="UTF-8")
hate
str(hate)
hate <- file(file.choose(), encoding="UTF-8")
head(hate)
head(hate)
hate
hate <- file(file.choose(), encoding="UTF-8")
library(tm)
library(wordcloud)
hate.txt <- readLines(hate)
head(hate.txt) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(hate.txt) # chr  [1:496]
obama.txt[76]
hate.txt.txt[76]
hate.txt[76]
#데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(hate.txt))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
# (2) 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, tolower) # 소문자 변경
myCorpusPrepro <-tm_map(myCorpusPrepro, removeWords, stopwords('english')) # 불용어제거
stopwords('english')
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 4. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(영문 1개 1byte) 단어 선별 -> matrix 변경
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(2,Inf))) # 2~무한대
myCorpusPrepro_term
# (2) Corpus -> 평서문 변환 : matrix -> data.frame 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
## 5. 단어 빈도수 구하기
# (1) 단어 빈도수 내림차순 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
# (2) 내림정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
## 6. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal)#, family="malgun")
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal)#, family="malgun")
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
head(hate.txt) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(hate.txt) # chr  [1:496]
#데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(hate.txt))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
myCorpusPrepro <-tm_map(myCorpusPrepro, removeWords, c(stopwords('english'),'rt')) # 불용어제거
stopwords('english')
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 4. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(영문 1개 1byte) 단어 선별 -> matrix 변경
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(2,Inf))) # 2~무한대
myCorpusPrepro_term
# (2) Corpus -> 평서문 변환 : matrix -> data.frame 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
## 5. 단어 빈도수 구하기
# (1) 단어 빈도수 내림차순 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
# (2) 내림정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
wordResult[1:10]
## 6. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal)#, family="malgun")
library(tm)
library(wordcloud)
hate <- file(file.choose(), encoding="UTF-8")
hate.txt <- readLines(hate)
head(hate.txt) # 앞부분 6줄 보기 - 줄 단위 문장 확인
hate <- file(file.choose(), encoding="UTF-8")
hate.txt <- readLines(hate)
head(hate.txt) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(hate.txt) # chr  [1:496]
#데이터 전처리
# (1) 말뭉치(코퍼스:Corpus) 생성 : 텍스트를 처리할 수 있는 자료의 집합
myCorpus <- Corpus(VectorSource(hate.txt))  # 벡터 소스 생성 -> 코퍼스 생성
myCorpus
inspect(myCorpus[1]) # corpus 내용 보기
inspect(myCorpus[2])
# (2) 데이터 전처리 : 말뭉치 대상 전처리
#tm_map(x, FUN)
myCorpusPrepro <- tm_map(myCorpus, removePunctuation) # 문장부호 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, removeNumbers) # 수치 제거
myCorpusPrepro <- tm_map(myCorpusPrepro, tolower) # 소문자 변경
myCorpusPrepro <- tm_map(myCorpusPrepro, removeWords, c(stopwords('english'),'rt')) # 불용어제거
inspect(myCorpusPrepro[1])
# (3) 전처리 결과 확인
myCorpusPrepro # Content:  documents: 76
inspect(myCorpusPrepro[1:5]) # 데이터 전처리 결과 확인(숫자, 영문 확인)
## 4. 단어 선별(단어 길이 2개 이상)
# (1) 단어길이 2개 이상(영문 1개 1byte) 단어 선별 -> matrix 변경
myCorpusPrepro_term <- TermDocumentMatrix(myCorpusPrepro,
control=list(wordLengths=c(2,Inf))) # 2~무한대
myCorpusPrepro_term
str(myCorpusPrepro_term)
# (2) Corpus -> 평서문 변환 : matrix -> data.frame 변경
myTerm_df <- as.data.frame(as.matrix(myCorpusPrepro_term))
## 5. 단어 빈도수 구하기
# (1) 단어 빈도수 내림차순 정렬
wordResult <- sort(rowSums(myTerm_df), decreasing=TRUE)
## 6. 단어구름에 디자인 적용(빈도수, 색상, 랜덤, 회전 등)
# (1) 단어 이름 생성 -> 빈도수의 이름
myName <- names(wordResult) # 단어이름 추출
# (2) 단어이름과 빈도수로 data.frame 생성
word.df <- data.frame(word=myName, freq=wordResult)
head(word.df)
str(word.df) # word, freq 변수
# (3) 단어 색상과 글꼴 지정
pal <- brewer.pal(12,"Paired") # 12가지 색상 pal <- brewer.pal(9,"Set1") # Set1~ Set3
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal), family="malgun")
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal), family="malgun")
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal, family="malgun")
# (4) 단어 구름 시각화 - 별도의 창에 색상, 빈도수, 글꼴, 회전 등의 속성을 적용하여
wordcloud(word.df$word, word.df$freq,
scale=c(5,1), min.freq=2, random.order=F,
rot.per=.1, colors=pal, family="malgun")
# 한글 처리를 위한 패키지 설치
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_151')
library(rJava) # 아래와 같은 Error 발생 시 Sys.setenv()함수로 java 경로 지정
library(KoNLP) # rJava 라이브러리가 필요함
# 1.텍스트 파일 가져오기
#----------------------------------------------------
marketing <- file("C:/ITWILL/2_Rwork/Part-II/marketing.txt", encoding="UTF-8")
marketing2 <- readLines(marketing) # 줄 단위 Test file 읽기
close(marketing)
head(marketing2) # 앞부분 6줄 보기 - 줄 단위 문장 확인
str(marketing2)
# 2. 줄 단위 단어 추출
#----------------------------------------------------
# Map()함수 이용 줄 단위 단어 추출
lword <- Map(extractNoun, marketing2)
lword <- Map(extractNoun, marketing2)
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(119개 제거)
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword)
class(lword) # list
lword # 추출 단어 확인
lword[1]
lword
lword[1]
# 4. 트랜잭션 생성 : 연관분석 위해서 트랜잭션 변환
#----------------------------------------------------
# arules 패키지 설치
install.packages("arules")
library(arules)
wordtran <- as(lword_final, "transactions") # 중복 word 있으면 error발생
wordtran
wordtran <- as(lword_final, "transactions") # 중복 word 있으면 error발생
library(arules)
wordtran <- as(lword_final, "transactions") # 중복 word 있으면 error발생
# 2) Filter(f,x) -> filter1() 함수를 적용하여 x 벡터 단위 필터링
filter2 <- function(x){
Filter(filter1, x)
}
# 3. 전처리
#----------------------------------------------------
# 1) 한글 단어 길이 2~4 필터링 함수
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
# 3) 줄 단어 대상 필터링
lword_final <- sapply(lword, filter2)
lword_final # 추출 단어 확인(길이 1개 단어 삭제됨)
wordtran <- as(lword_final, "transactions") # 중복 word 있으면 error발생
wordtran
# 트랜잭션 내용 보기 -> 각 트랜잭션의 단어 보기
inspect(wordtran)
# 5.단어 간 연관규칙 산출
#----------------------------------------------------
# 지지도와 신뢰도를 적용하여 연관규칙 생성
# 형식) apriori(data, parameter = NULL, appearance = NULL, control = NULL)
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
inspect(tranrules) # 연관규칙 생성 결과
tranrules #
# (2) 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
# 6.연관어 시각화
#----------------------------------------------------
# (1) 데이터 구조 변경 : 연관규칙 -> label 추출
rules <- labels(tranrules, ruleSep=" ")
rules
# 문자열로 묶인 연관단어를 행렬구조 변경
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rules
# list -> matrix 반환
rulemat <- do.call("rbind", rules)
rulemat
# (2) 연관어 시각화를 위한 igraph 패키지 설치
install.packages("igraph") # graph.edgelist(), plot.igraph(), closeness() 함수 제공
library(igraph)
# (3) edgelist보기 - 연관단어를 정점 형태의 목록 제공
ruleg <- graph.edgelist(rulemat[c(12:59),], directed=F) # [1,]~[11,] "{}" 제외
ruleg
# (4) edgelist 시각화
X11()
# (4) edgelist 시각화
X11()
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
library(ggmap)
get_googlemap('Losangeles',zoom=15,maptype="roadmap")
# 주사위 나올 수 있는 확률이 1/6이 아닐 때(3,4번이 많이 나올 때)
p <- c(0.1, 0.1, 0.25, 0.25, 0.1, 0.2)
chisq.test(c(4,6,17,16,8,9), p=p)
# 1. 실습데이터 가져오기
setwd("C:/ITWILL/2_Rwork/Part-III")
# 1. 실습데이터 가져오기
data <- read.csv("two_sample.csv", header=TRUE)
data
head(data) # 변수명 확인
# 2. 두 집단 subset 작성
data$method # 1, 2 -> 노이즈 없음
data$survey # 1(만족), 0(불만족)
# - 데이터 정체/전처리
x<- data$method # 교육방법(1, 2) -> 노이즈 없음
y<- data$survey # 만족도(1: 만족, 0:불만족)
x;y
# 양측가설 검정
prop.test(c(110,135), c(150, 150)) # 14와 20% 불만족율 기준 차이 검정
prop.test(c(110,135), c(150, 150), alternative="two.sided", conf.level=0.95)
chisq.test(data$method, data$survey) #같은 결과가 나옴.
setwd("c:/Rwork/Part-III")
data <- read.csv("paired_sample.csv", header=TRUE)
# 1) 데이터 정제
#result <- subset(data, !is.na(after), c(before,after))
dataset <- data[ c('before',  'after')]
dataset
head(data)
# 1) 데이터 정제
#result <- subset(data, !is.na(after), c(before,after))
dataset <- data[ c('before',  'after')]
dataset
# 2) 적용전과 적용후 분리
before <- dataset$before# 교수법 적용전 점수
after <- dataset$after # 교수법 적용후 점수
before; after
# 3) 기술통계량
length(before) # 100
length(after) # 100
mean(before) # 5.145
mean(after, na.rm = T) # 6.220833 -> 1.052  정도 증가
# 3. 분포모양 검정
var.test(before, after, paired=TRUE)
# 1. 파일가져오기
data <- read.csv("three_sample.csv", header=TRUE)
data
# 2. 두 집단 이상 subset 작성(데이터 정제,전처리)
method <- data$method
survey<- data$survey
method
survey
# 3.기술통계량(빈도분석)
table(method, useNA="ifany") # 50 50 50 -> 3그룹 모두 관찰치 50개
table(method, survey, useNA="ifany") # 그룹별 클릭수 : 1-43, 2-34, 3-37
# 4. 두 집단 이상 비율차이 검정
# prop.test(그룹별 빈도, 그룹수) -> 집단이 늘어나도 동일한 함수 사용-땡큐
prop.test(c(34,37,39), c(50,50,50)) # p-value = 0.1165 -> 귀무가설 채택
# 1. 파일 가져오기
data <- read.csv("three_sample.csv")
# 2. 데이터 정제/전처리 - NA, outline 제거
data <- subset(data, !is.na(score), c(method, score))
data # method, score
# (1) 차트이용 - ontlier 보기(데이터 분포 현황 분석)
plot(data$score) # 차트로 outlier 확인 : 50이상과 음수값
barplot(data$score) # 바 차트
mean(data$score) # 14.45
# (2) outlier 제거 - 평균(14) 이상 제거
length(data$score)#91
data2 <- subset(data, score <= 14) # 14이상 제거
length(data2$score) #88(3개 제거)
# (3) 정제된 데이터 보기
x <- data2$score
boxplot(x)
plot(x)
boxplot(data$score)$stats
# (3) 정제된 데이터 보기
x <- data2$score
boxplot(x)
plot(x)
# 3. 집단별 subset 작성
# method: 1:방법1, 2:방법2, 3:방법3
data2$method2[data2$method==1] <- "방법1"
data2$method2[data2$method==2] <- "방법2"
data2$method2[data2$method==3] <- "방법3"
table(data2$method2) # 교육방법 별 빈도수
# 4. 동질성 검정 - 정규성 검정
# bartlett.test(종속변수 ~ 독립변수) # 독립변수(세 집단)
# 종속변수 ~ 독립변수 이런 형식을 포뮬라라고 함.
bartlett.test(score ~ method2, data=data2)
# 귀무가설 : 집단 간 평균에 차이가 없다.
result <- aov(score ~ method2, data=data2)
result
# aov()의 결과값은 summary()함수를 사용해야 p-value 확인
summary(result)
TukeyHSD(result) # 분석분석의 결과로 사후검정
plot(TukeyHSD(result))
str(iris)
names(iris)
# 1. 동질성 검정 : 전제조건
bartlett.test(iris$Sepal.Lenght, iris$Species)
# 1. 동질성 검정 : 전제조건
bartlett.test(iris$Sepal.Lenght, iris$Species)
# 1. 동질성 검정 : 전제조건
bartlett.test(iris$Sepal.Length, iris$Species)
# p-value = 0.0003345
bartlett.test(iris$Sepal.Width, iris$Species)
# 2. 변수 선택 (변수 선택은 아주 중요함!)
x <- iris$Sepal.Width
y <- iris$Species
# 3. 분산분석
result <- aov(Sepal.Width ~ Species, data = iris)
summary(result)
# 4. 사후검정
TukeyHSD(result)
plot(TukeyHSD(result))
#plot은 만능함수
method(plot)
#plot은 만능함수
methods(plot)
library(dplyr)
iris %>% group_by(Species)
iris %>% group_by(Species) %>% summarise(age = mean(Sepal.Width))
2.77 - 3.43 #versicolor-setosa
# 1. dataset 생성
age <- runif(100, min=20, max=59)
# 1. dataset 생성
age <- round(runif(100, min=20, max=59)) #round 반올림해서 실수를 정수로 만듦
age
time <- round(runif(100, min=0, max=1))
time
buy <- round(runif(100, min=1, max=10))
data <- data.frame(age, time, buy)
# 연속형->범주형
daat$age2[data$age <= 29] <- 20
daat$age2[data$age > 30 & data$age <= 39] <- 30
data <- data.frame(age, time, buy)
# 연속형->범주형
daat$age2[data$age <= 29] <- 20
daat$age2[data$age > 30 & data$age <= 39] <- 30
daat$age2[data$age > 40 & data$age <= 49] <- 40
# 연속형->범주형
data$age2[data$age <= 29] <- 20
data$age2[data$age > 30 & data$age <= 39] <- 30
data$age2[data$age > 40 & data$age <= 49] <- 40
data$age2[data$age > 49] <- 50
head(data)
# 2. 동질성 검정
bartlett.test(buy ~ age2, data = data)
bartlett.test(buy ~ time, data = data)
aov(formula = buy ~ age2 + time, data = data)
result <- aov(formula = buy ~ age2 + time, data = data)
summary(result)
summary(result)
# 4. 사후검정
TukeyHSE(result)
# 4. 사후검정
TukeyHSD(result)
# 4. 사후검정 : 집단별 분산 차이
library(dplyr)
data %>% group_by(age2) %>% summarise(buy_age = mean(buy))
# 연속형->범주형
data$age2[data$age <= 29] <- 20
data$age2[data$age >= 30 & data$age <= 39] <- 30
data$age2[data$age >= 40 & data$age <= 49] <- 40
data$age2[data$age >= 49] <- 50
head(data)
# 2. 동질성 검정
bartlett.test(buy ~ age2, data = data)
bartlett.test(buy ~ time, data = data)
result <- aov(formula = buy ~ age2 + time, data = data)
summary(result)
# 4. 사후검정 : 집단별 분산 차이
library(dplyr)
data %>% group_by(age2) %>% summarise(buy_age = mean(buy))
TukeyHSD(result)
data %>% group_by(time) %>% summarise(buy_time = mean(buy))
product <- read.csv("C:/ITWILL/2_Rwork/Part-III/product.csv", header=TRUE)
head(product) # 친밀도 적절성 만족도(등간척도 - 5점 척도)
product <- read.csv("C:/ITWILL/2_Rwork/Part-III/product.csv", header=TRUE)
head(product) # 친밀도 적절성 만족도(등간척도 - 5점 척도)
# 기술통계량
summary(product) # 요약통계량
# 1) 상관계수(coefficient of correlation) : 두 변량 X,Y 사이의 상관관계 정도를 나타내는 수치(계수)
cor(product$제품_친밀도, product$제품_적절성) # 0.4992086 -> 다소 높은 양의 상관관계
cor(product$제품_친밀도, product$제품_만족도) # 0.467145 -> 다소 높은 양의 상관관계
# 전체 변수 간 상관계수 보기
cor(product, method="pearson") # 피어슨 상관계수 - default
cor(product$제품_적절성, product$제품_만족도)
# 방향성 있는 색생으로 표현 - 동일 색상으로 그룹화 표시 및 색의 농도
install.packages("corrgram")
library(corrgram)
corrgram(product) # 색상 적용 - 동일 색상으로 그룹화 표시
corrgram(product, upper.panel=panel.conf) # 수치(상관계수) 추가(위쪽)
corrgram(product, upper.panel=panel.conf) # 수치(상관계수) 추가(위쪽)
corrgram(product, lower.panel=panel.conf) # 수치(상관계수) 추가(아래쪽)
# 차트에 곡선과 별표 추가
install.packages("PerformanceAnalytics")
# 상관성,p값(*),정규분포 시각화 - 모수 검정 조건
chart.Correlation(product, histogram=, pch="+")
library(PerformanceAnalytics)
# 상관성,p값(*),정규분포 시각화 - 모수 검정 조건
chart.Correlation(product, histogram=, pch="+")
# 2) 공분산(covariance) : 두 변량 X,Y의 관계를 나타내는 양
cov(product)
# 2) 공분산(covariance) : 두 변량 X,Y의 관계를 나타내는 양
cor(product)
cov(product)
cov2cor(cov(product)) # 공분산 행렬 -> 상관계수 행렬 변환
x <- product$제품_적절성
y <- product$제품_만족도
Cov_xy
x <- product$제품_적절성
y <- product$제품_만족도
Cov_xy
x <- product$제품_적절성
y <- product$제품_만족도
x <- product$제품_적절성
y <- product$제품_만족도
x <- product$제품_적절성
y <- product$제품_만족도
x <- product$제품_적절성
y <- product$제품_만족도
x <- product$제품_적절성
y <- product$제품_만족도
x <- product$제품_적절성
y <- product$제품_만족도
x;y
x <- product$제품_적절성
y <- product$제품_만족도
Cov_xy <- mean((x-mean(X)) * (y-mean(y))) #편차
x <- product$제품_적절성
y <- product$제품_만족도
Cov_xy <- mean((x-mean(x)) * (y-mean(y))) #편차
Cov_xy
cov(product)
# 2) 공분산(covariance) : 두 변량 X,Y의 관계를 나타내는 양
cor(product)
